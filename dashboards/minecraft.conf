{
  duration: "4h",
  tabs: [{"tabName":"Overview","graphs":[
    {
      query: "(logfile contains 'minecraft_mc-forge_1') \"logged in with entity id\" \"Server thread/INFO\" \n\n\n| parse \": $username$\\\\[\\/$ip$:$port$\\]\"\n| group Logins = count() by ip, username",
      title: "Logins ",
      graphStyle: "",
      showBarsColumn: "false",
      layout: {
  h: 14,
  w: 20,
  x: 0,
  y: 6
}
    },
    {
      query: "(logfile contains 'minecraft' || k8s-controller contains 'minecraft' || k8s-cron-job contains 'minecraft' || k8s-daemon-set contains 'minecraft' || k8s-deployment contains 'minecraft' || k8s-job contains 'minecraft' || k8s-replica-set contains 'minecraft' || k8s-replication-controller contains 'minecraft' || k8s-stateful-set contains 'minecraft') Preparing level  \"[Server thread/INFO]:\"\n|parse \"Preparing level \\\"$level$\\\"\"\n| group Levels = count() ",
      title: "Levels Loaded",
      graphStyle: "number",
      showBarsColumn: "false",
      layout: {
  h: 5,
  w: 10,
  x: 20,
  y: 0
}
    },
    {
      query: "|join\nserver_logins = (\n(logfile contains 'minecraft') \"logged in with entity id\" \"Server thread/INFO\"\n|sort timestamp\n| parse \": $username$\\\\[\\/$ip$:$port$\\]\"\n| group Logins = count() by ip, username, port\n),\n\nvpc_connections = (\n\n\naction='ACCEPT' logfile='minecraft' serverHost='cloudwatch-630972250024' dstaddr = \"172.31.17.202\" \n| sort timestamp\t\n|columns dstaddr, srcaddr, srcport, dstport, srcport, version, account-id, interface-id, protocol, packets, bytes, start, end, action, timestamp\n| group count() by  dstaddr, ip=srcaddr, srcport, dstport = dstport+\"\", srcport, version, account-id= account-id+\"\", interface-id, protocol, packets, bytes, start, end, action\n\n)\n\non vpc_connections.ip = server_logins.ip, vpc_connections.srcport = server_logins.port",
      title: "User Connections Correlation (VPC Flow+Server Log) ",
      layout: {
  h: 14,
  w: 20,
  x: 40,
  y: 6
}
    },
    {
      query: "(logfile contains 'minecraft_mc-forge_1') \"logged in with entity id\" \"Server thread/INFO\" \n\n\n| parse \": $username$\\\\[\\/$ip$:$port$\\]\"\n| group Logins = count() ",
      title: "Logins",
      graphStyle: "number",
      layout: {
  h: 5,
  w: 20,
  x: 0,
  y: 0
}
    },
    {
      layout: {
  h: 5,
  i: "4",
  minH: 3,
  minW: 6,
  w: 30,
  x: 30,
  y: 0
},
      query: "|join\nserver_logins = (\n \"logged in with entity id\" \"Server thread\"\n|sort timestamp\n| parse \": $username$\\\\[\\/$ip$:$port$\\]\"\n| group Logins = count() by ip, username, port\n),\n\nvpc_connections = (\n\n\naction='ACCEPT' dstaddr = \"172.31.17.202\" \n| sort timestamp\t\n|columns dstaddr, srcaddr, srcport, dstport, srcport, version, account-id, interface-id, protocol, packets, bytes, start, end, action, timestamp\n| group count() by  dstaddr, ip=srcaddr, srcport, dstport = dstport+\"\", srcport, version, account-id= account-id+\"\", interface-id, protocol, packets, bytes, start, end, action\n\n)\n\non vpc_connections.ip = server_logins.ip, vpc_connections.srcport = server_logins.port\n| group Public_Logins = count()",
      title: "User Connections Correlation (VPC Flow+Server Log) ",
      graphStyle: "number",
      showBarsColumn: "false"
    },
    {
      label: "Total CPU Usage",
      plots: [
        {
          facet: "value",
          filter: "mean(value where metric == \"docker.cpu.total_usage_rate\"  ) / 1000000000",
          label: "Total CPU usage (% of 1 core)"
        }
      ],
      layout: {
  h: 14,
  w: 20,
  x: 20,
  y: 6
}
    },
    {
      query: "\n \"logged in with entity id\" \"Server thread\"\n|sort timestamp\n| parse \": $username$\\\\[\\/$ip$:$port$\\]\"\n| group Logins = count() by ip, username, port\n",
      title: "User Logins"
    },
  ]},
{"tabName":"Game",
graphs : [
    {
      barWidth: "auto",
      breakdownFacet: "player",
      graphStyle: "stacked_bar",
      plots: [
        {
          facet: "rate",
          filter: "action=\"death\"",
          label: "rate"
        }
      ],
      title: "Deaths",
      yScale: "linear",
      layout: {
        h: 14,
        w: 20,
        x: 0,
        y: 0
      }
    },
    {
      barWidth: "auto",
      breakdownFacet: "killer",
      graphStyle: "stacked_bar",
      plots: [
        {
          facet: "rate",
          filter: "action=\"death\"",
          label: "rate"
        }
      ],
      title: "Causes of Death",
      layout: {
        h: 14,
        w: 20,
        x: 20,
        y: 0
      }
    },
    {
      barWidth: "auto",
      breakdownFacet: "block",
      graphStyle: "stacked_bar",
      plots: [
        {
          facet: "rate",
          filter: "action='block place'",
          label: "rate"
        }
      ],
      title: "Blocks Placed",
      yScale: "linear",
      layout: {
  h: 14,
  w: 20,
  x: 40,
  y: 14
}
    },
    {
      barWidth: "auto",
      breakdownFacet: "player",
      graphStyle: "stacked_bar",
      plots: [
        {
          facet: "rate",
          filter: "player=*",
          label: "rate"
        }
      ],
      title: "Actions by Player",
      layout: {
        h: 14,
        w: 20,
        x: 0,
        y: 14
      }
    },
    {
      query: "action = \"death\"\n| group deaths=count() by cause, player | sort player",
      title: "Death Count",
      layout: {
        h: 14,
        w: 20,
        x: 20,
        y: 14
      }
    },
    {
      breakdownFacet: "player",
      graphStyle: "line",
      plots: [
        {
          facet: "count",
          filter: "action='item pickup'",
          label: "count"
        }
      ],
      title: "Items Picked Up by Player",
      layout: {
  h: 14,
  w: 20,
  x: 40,
  y: 28
}
    },
    {
      query: "action contains \"chat\" !(message_ contains \"REPORT\")\n| columns time, timestamp, player, message_\n| sort timestamp\n| let chat = time + \" <\" + player + \"> \" + message_\n| columns chat\n\n",
      title: "Chat",
      layout: {
  h: 25,
  w: 40,
  x: 0,
  y: 28
}
    },
    {
      query: "message_ contains \"REPORT\"\n| parse \"REPORT: $reportedUser$, *\" from message_\n| group reports=count() by reportedUser, player\n| columns \"Reported User\"=reportedUser, \"Reported By\"=player, \"Total Reports\"=reports",
      title: "Abusive Players",
      layout: {
  h: 23,
  w: 20,
  x: 40,
  y: 42
}
    },
    {
      query: "message_=*\n| lookup type from \"curses.csv\" by curse=message_ | filter type = *\n| group castCount=count() by player, message_, type\n| columns \"Player\"=player, \"Curse\"=message_, \"Type\"=type, \"Cast Amount\"=castCount",
      title: "Spells",
      layout: {
  h: 12,
  w: 40,
  x: 0,
  y: 53
}
    },
    {
      barWidth: "auto",
      breakdownFacet: "player",
      graphStyle: "stacked_bar",
      plots: [
        {
          facet: "rate",
          filter: "player=* action='block break'",
          label: "rate"
        }
      ],
      title: "Blocks Broken",
      layout: {
        h: 14,
        w: 20,
        x: 40,
        y: 0
      }
    }
  ]},
{"tabName":"Docker",parameters : [

      ],

      // duration: "4h",

      graphs: [
        {
          label: "User vs System CPU Usage",
          plots: [
            {
              facet: "value",
              filter: "mean(value where metric == \"docker.cpu.usage_in_usermode_rate\"  ) / 1000000000",
              label: "Usermode CPU usage (% of 1 core)"
            }, {
              facet: "value",
              filter: "mean(value where metric == \"docker.cpu.usage_in_kernelmode_rate\"  ) / 1000000000",
              label: "Kernelmode CPU usage (% of 1 core)"
            }
          ],
          layout: {
  h: 14,
  w: 20,
  x: 0,
  y: 7
}
        },{
          label: "Memory Usage",
          plots: [
            {
              facet: "value",
              filter: "mean(value where metric == \"docker.mem.usage\"  ) / 1048576",
              label: "Memory Usage (MB)"
            },{
              facet: "value",
              filter: "mean(value where metric == \"docker.mem.limit\"  ) / 1048576",
              label: "Memory Limit (MB)"
            }
          ],
          layout: {
  h: 14,
  w: 20,
  x: 20,
  y: 7
}
        }, {
          label: "Network bandwidth",
          plots: [
            {
              facet: "value",
              filter: "mean(value where metric == \"docker.net.rx_bytes_rate\"   && interface = \"eth0\")",
              label: "Received bytes / sec"
            },{
              facet: "value",
              filter: "mean(value where metric == \"docker.net.tx_bytes_rate\"   && interface = \"eth0\")",
              label: "Sent bytes / sec"
            }
          ],
          layout: {
  h: 14,
  w: 20,
  x: 40,
  y: 7
}
        }, {
          label: "Network packets",
          plots: [
            {
              facet: "value",
              filter: "mean(value where metric == \"docker.net.rx_packets_rate\"   && interface = \"eth0\")",
              label: "Received packets / sec"
            },{
              facet: "value",
              filter: "mean(value where metric == \"docker.net.tx_packets_rate\"   && interface = \"eth0\")",
              label: "Sent packets / sec"
            }
          ],
          layout: {
  h: 14,
  w: 20,
  x: 0,
  y: 21
}
        }, {
          label: "Network errors",
          plots: [
            {
              facet: "value",
              filter: "mean(value where metric == \"docker.net.rx_dropped_rate\"   && interface = \"eth0\")",
              label: "Dropped incoming packets / sec"
            },{
              facet: "value",
              filter: "mean(value where metric == \"docker.net.tx_dropped_rate\"   && interface = \"eth0\")",
              label: "Dropped outgoing packets / sec"
            },         {
              facet: "value",
              filter: "mean(value where metric == \"docker.net.rx_errors_rate\"   && interface = \"eth0\")",
              label: "Receive errors / sec"
            },{
              facet: "value",
              filter: "mean(value where metric == \"docker.net.tx_errors_rate\"   && interface = \"eth0\")",
              label: "Trasmit errors / sec"
            }
          ],
          layout: {
  h: 14,
  w: 20,
  x: 20,
  y: 21
}
        },
        {
          label: "Total CPU Usage",
          plots: [
            {
              facet: "value",
              filter: "mean(value where metric == \"docker.cpu.total_usage_rate\"  ) / 1000000000",
              label: "Total CPU usage (% of 1 core)"
            }
          ],
          layout: {
  h: 14,
  w: 20,
  x: 40,
  y: 21
}
        },
        {
          graphStyle: "line",
          plots: [{filter: "(logfile contains 'docker' || k8s-controller contains 'docker' || k8s-cron-job contains 'docker' || k8s-daemon-set contains 'docker' || k8s-deployment contains 'docker' || k8s-job contains 'docker' || k8s-replica-set contains 'docker' || k8s-replication-controller contains 'docker' || k8s-stateful-set contains 'docker') monitor != 'docker_monitor'"}],
          title: "Logs",
          layout: {
  h: 7,
  w: 60,
  x: 0,
  y: 0
}
        },
        {
          query: "(logfile contains 'docker' || k8s-controller contains 'docker' || k8s-cron-job contains 'docker' || k8s-daemon-set contains 'docker' || k8s-deployment contains 'docker' || k8s-job contains 'docker' || k8s-replica-set contains 'docker' || k8s-replication-controller contains 'docker' || k8s-stateful-set contains 'docker') monitor != 'docker_monitor'\n| columns timestamp, message\n| sort -timestamp",
          title: "Logs",
          layout: {
  h: 12,
  w: 60,
  x: 0,
  y: 35
}
        }
      ],
      filters: [
        {
          defaultValue: "",
          facet: "instance",
          name: "instance"
        }
      ]
    },
{"tabName":"AWS","graphs":[
  {
    query: "action='ACCEPT'  dstaddr = \"172.31.17.202\" dstport = 25565\t\n|columns dstaddr, srcaddr, srcport, dstport, srcport, version, account-id, interface-id, protocol, packets, bytes, start, end, action, timestamp\n| group count() by timestamp = timebucket(\"1m\"), dstaddr, srcaddr, srcport, dstport, srcport, version, account-id, interface-id, protocol, packets, bytes, start, end, action",
    title: "VPC Flow Log",
    layout: {
  h: 14,
  w: 29,
  x: 0,
  y: 14
}
  },
  {
    query: "action='ACCEPT' | sort -timestamp | limit 10 | group count() by dstaddr, timestamp, action, srcaddr, srcport, dstport, srcport",
    title: "Connections",
    layout: {
  h: 14,
  w: 31,
  x: 29,
  y: 14
},
    graphStyle: "",
    showBarsColumn: "false"
  },
  {
    graphStyle: "line",
    lineSmoothing: "straightLines",
    query: "metric='CPUUtilization' logfile='cloudwatch' serverHost='cloudwatch-us-west-1'\n| group max(value) by timestamp = timebucket(\"1m\")",
    title: "CloudWatch Metrics CPUUtilization ",
    yScale: "linear",
    layout: {
  h: 14,
  w: 20,
  x: 0,
  y: 0
}
  },
  {
    graphStyle: "line",
    lineSmoothing: "straightLines",
    query: "metric='NetworkIn' logfile='cloudwatch' serverHost='cloudwatch-us-west-1'\n| group max(value) by timestamp = timebucket(\"1m\")",
    title: "CloudWatch Metrics NetworkIn",
    yScale: "linear",
    layout: {
  h: 14,
  w: 20,
  x: 40,
  y: 0
}
  },
  {
    graphStyle: "line",
    lineSmoothing: "straightLines",
    query: "metric='NetworkOut' logfile='cloudwatch' serverHost='cloudwatch-us-west-1'\n| group max(value) by timestamp = timebucket(\"1m\")",
    title: "CloudWatch Metrics NetworkOut",
    yScale: "linear",
    layout: {
  h: 14,
  w: 20,
  x: 20,
  y: 0
}
  },
  {
    query: "'MessageService'|group count() by logfile, message",
    title: "CloudWatch Agent Logs",
    layout: {
  h: 14,
  w: 60,
  x: 0,
  y: 28
}
  }
]},
{"tabName":"Security","graphs":[
  {
    graphStyle: "line",
    plots: [{filter: "endpoint.name='ip-172-31-17-202.us-west-1.compute.internal' dataSource.category = 'security' dataSource.name = 'SentinelOne'"}],
    teamEmails: ["11633_426418030212073761@s1.oem"],
    title: "Security Logs"
  }
]},
{"tabName":"Network","graphs":[
  {
    graphStyle: "line",
    plots: [{filter: "srcaddr = *  dstaddr = * "}],
    title: "Connections lookup",
    layout: {
  h: 12,
  w: 60,
  x: 0,
  y: 0
}
  },
  {
    query: "srcaddr = \"172.31.13.57\" ACCEPT dstaddr in (\"172.31.17.202\", \"192.168.224.1\", \"192.168.192.1\", \"172.17.0.1\") \n|group count() by dstaddr, srcaddr, dstport = dstport + \"\", srcport = srcport + \"\", action \n| columns dstaddr, srcaddr, dstport, srcport, action\n",
    teamEmails: ["11633_426418030212073761@s1.oem"],
    title: "Network",
    layout: {
  h: 23,
  w: 60,
  x: 0,
  y: 12
}
  }
],
filters: [
  {
    facet: "dstaddr",
    name: "dstaddr",
    defaultValue: "172.31.17.202"
  },
  {
    facet: "srcport",
    name: ""
  },
  {
    facet: "srcport",
    name: "srcport"
  },
  {
    facet: "srcaddr",
    name: "srcaddr"
  },
  {
    facet: "dstport",
    name: "dstport"
  },
  {
    facet: "action",
    name: "action"
  }
]
},
{"tabName":"EC2","graphs":[   // CPU load.
    // loadavg.5min is also available, but we leave it out here as it adds to clutter.
    {"facet": "value","label": "CPU load average","plots": [
	  {"filter": "$source='tsdb'  metric='proc.loadavg.1min' ", "label": "1 min avg"},
	  {"filter": "$source='tsdb'  metric='proc.loadavg.15min' ", "label": "15 min avg"}]},
	
	// CPU graph
  // Other CPU types that we're not displaying (to minimize clutter): idle, irq, guest, guest_nice
	{facet: "value", label: "CPU usage", graphStyle: "stacked", plots: [
	  {"filter": "$source='tsdb'  metric='proc.stat.cpu_rate' type='user'", "label": "user"},
	  {"filter": "$source='tsdb'  metric='proc.stat.cpu_rate' type='system'", "label": "system"},
	  {"filter": "$source='tsdb'  metric='proc.stat.cpu_rate' type='nice'", "label": "nice"},
	  {"filter": "$source='tsdb'  metric='proc.stat.cpu_rate' type='iowait'", "label": "iowait"}]},
	
	// Disk usage. Note, this only covers the '/' mount point. Feel free to customize the graph to include other mount points.
	{facet: "value", label: "Disk usage (KB)", graphStyle: "stacked", plots: [
	  {"filter": "$source='tsdb'  metric='df.1kblocks.free' mount='/'", "label": "Free blocks (root volume)"},
	  {"filter": "$source='tsdb'  metric='df.1kblocks.used' mount='/'", "label": "Used blocks (root volume)"}
	]},
	
	// Used and free inodes. Note that we don't specify a stacked graph here, because that doesn't work well for sparse data,
	// and inode counts are only logged every 5 minutes.
  {facet: "value", label: "Disk usage (inodes)", plots: [
    {"filter": "$source='tsdb'  metric='df.inodes.free' mount='/'", "label": "Free inodes (root volume)"},
    {"filter": "$source='tsdb'  metric='df.inodes.used' mount='/'", "label": "Used inodes (root volume)"}
  ]},
  
	// Memory usage. We omit swapcached and swapfree to minimize clutter. 
	{facet: "value", label: "Memory usage (KB)", graphStyle: "stacked", plots: [
	  {"filter": "$source='tsdb'  metric='proc.meminfo.memfree' ", "label": "Free"},
	  {"filter": "$source='tsdb'  metric='proc.meminfo.memtotal' ", "label": "Total"},
	  {"filter": "$source='tsdb'  metric='proc.meminfo.swaptotal' ", "label": "Swap"},
	  {"filter": "$source='tsdb'  metric='proc.meminfo.buffers' ", "label": "Buffers"}
	]},
	
	// Network usage
	{"facet": "value", "label": "Network bandwidth", "graphStyle": "stacked", "plots": [
	  {"filter": "$source='tsdb'  metric='proc.net.bytes_rate' direction='in'", "label": "Received bytes/sec"},
	  {"filter": "$source='tsdb'  metric='proc.net.bytes_rate' direction='out'", "label": "Sent bytes/sec"}]},
	
	// Disk request rates
	{"facet": "value","label": "Disk request rates","plots": [
	  {"filter": "$source='tsdb'  metric='iostat.part.read_requests_rate'", "label": "Reads/s"},
	  {"filter": "$source='tsdb'  metric='iostat.part.write_requests_rate'", "label": "Writes/s"}
	]},

  // Disk request bandwidth
  {"facet": "sumPerSecond(value)", "label": "Disk request bandwidth", "plots": [
    { "filter": "sumPerSecond(value where $source='tsdb'  metric='iostat.disk.write_sectors_rate') * .00048828125", "label": "Write MB/s"},
    { "filter": "sumPerSecond(value where $source='tsdb'  metric='iostat.disk.read_sectors_rate') * .00048828125", "label": "Read MB/s"}
  ]},

	// Disk latency
	{"facet": "value","label": "Average disk request latency (ms)","plots": [
	  {"filter": "$source='tsdb'  metric='iostat.part.msec_read_rate'", "label": "Read"},
	  {"filter": "$source='tsdb'  metric='iostat.part.msec_write_rate'", "label": "Write"}
	]}],
filters: [
  {
    defaultValue: "ip-172-31-17-202.us-west-1.compute.internal",
    facet: "serverHost",
    name: "Server Name"
  }
]
}],
  configType: "TABBED"
}
